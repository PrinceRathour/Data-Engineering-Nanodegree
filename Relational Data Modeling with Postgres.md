## Introduction

A new music streaming application developed by a startup company called Sparkify. Associated with the application usage is the data of songs and user acitivity.
Data is valuable, in order to gain insights from the collect data of songs and user activities, the data is to be analyzed.

Sparkify's analytical team is keen in determing the user's behavior, the songs that they are listing and the likelihood of liking a song. As per the current configuration of data storage, 
which is in directory of json logs on user activity and as well on songs details, analyzing is a tedious process but with increase in json logs and metadata the difficulty will increase dramatically.



## Dataset Information


#### Song Dataset
The first dataset is a subset of real data from the [Million Song Dataset](https://labrosa.ee.columbia.edu/millionsong). Each file is in JSON format and contains metadata about a song and the artist of that song. The files are partitioned by the first three letters of each song's track ID. For example, here are filepaths to two files in this dataset.

```
song_data/A/B/C/TRABCEI128F424C983.json
song_data/A/A/B/TRAABJL12903CDCF1A.json
```
A single song file, TRAABJL12903CDCF1A.json, looks like. 
```{"num_songs": 1, "artist_id": "ARJIE2Y1187B994AB7", "artist_latitude": null, "artist_longitude": null, "artist_location": "", "artist_name": "Line Renaud", "song_id": "SOUPIRU12A6D4FA1E1", "title": "Der Kleine Dompfaff", "duration": 152.92036, "year": 0}```


#### Log Dataset

The second dataset consists of log files in JSON format generated by this [event simulator](https://github.com/Interana/eventsim) based on the songs in the dataset above. These simulate activity logs from a music streaming app based on specified configurations.

The log files in the dataset you'll be working with are partitioned by year and month. For example, here are filepaths to two files in this dataset.
```
log_data/2018/11/2018-11-12-events.json
log_data/2018/11/2018-11-13-events.json
```


#### Database STAR Schema
In order to overcome this complexity, creation of database is recommeded which is achieved by data modeling and creation of ETL pipelines.
An optimized STAR schema is needed to be created to analyze:-
* Top trending songs
* Recommendation of songs
* How likely is a user going to subscribe to the application

A Star Schema will have:
* **Fact Table** :- **songplays** 
* **Dimension Tables** :- **users**, **songs**, **artists**, **time**


**Tables Information** 

**songplays** : records in log data
    
    
| Column Name  | Variable |
| --- | --- |
| songplay_id   | int  |
| start_time    | timestamp  |
| user_id       | int  |
| level         | varchar  |
| song_id       | varchar  |
| artist_id     | varchar  |
| session_id    | int  |
| location      | text  |
| user_agent    | text  |

**users** : users in the app

| Column Name  | Variable |
| --- | --- |
|  user_id  |   int  |
|  first_name  |  varchar  |
|  last_name  |  varchar  |
|  gender  |  char  |
|  level  |  varchar  |

**songs** : songs in music database
| Column Name  | Variable |
| --- | --- |
|  song_id  |   varchar  |
|  title  |  text  |
|  artist_id  |  varchar  |
|  year  |  int  |
|  duration  |  float  |


**artists** :  artists in music database

| Column Name  | Variable |
| --- | --- |
|  artist_id  |   varchar  |
|  name  |  text  |
|  location  |  varchar  |
|  latitude  |  numeric  |
|  longitude  |  numeric  |

**time** :  timestamps of records in songplays broken down into specific units

| Column Name  | Variable |
| --- | --- |
|  start_time  |   timestamp  |
|  hour  |  int  |
|  day  |  int  |
|  week  |  int  |
|  month  |  int  |
|  year  |  int  |
|  weekday  |  varchar  |


## ETL Process and Pipeline Creation
* **songplays**       : Fact table created from extraction of attributes from song data and log data json files.
* **songs , artists** : Dimension tables created from extraction of attributes from song data json file.
* **users , time**    : Dimension tables created from extraction of attributes from log data json file.

**Note:-**  
**etl.py** : Performs Extraction of data from the log json files and song data files, Transform the data as per the required  structure of the table and Load in the specified tables. 




## Libraries Required


**Python Libraries Used** : 
1. **os**       : [os — Miscellaneous operating system interfaces](https://docs.python.org/3/library/os.html)
2. **glob**     : [glob — Unix style pathname pattern expansion](https://docs.python.org/3/library/glob.html)
3. **psycopg2** : [psycopg2 - Python-PostgreSQL Database Adapter](https://pypi.org/project/psycopg2/)
4. **pandas**   : [Powerful data structures for data analysis, time series, and statistics](https://pypi.org/project/pandas/)


**Installation on Ipynb console**

```python

!pip install glob
!pip install psycopg2
!pip install pandas
```
     

## Files Usage 


 displays the first few rows of each table to let you check your database.
1. **create_tables.py** : Drops existing tables if exists and creates tables. Run this file every time before executing ETL.py.
2. **etl.ipynb**        : It extracts, transforms and loads a single file from song data and log data into the tables.
3. **etl.py**           : It extracts, transforms and loads all files from song data and log data into the tables.
4. **sql_queries.py**   : It contains all SQL queries from creation of database to inserting values into tables.
2. **test.ipynb**       : It run only after executing the create_tables script to show the table struture.

   
### **Note** :- Execute files in the following order

   1. create_tables.py
      
   2. etl.ipynb or etlt.py
      
   3. test.ipynb
